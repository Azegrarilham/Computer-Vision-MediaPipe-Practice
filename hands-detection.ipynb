{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f52a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b86cd",
   "metadata": {},
   "source": [
    "# Hand detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e37e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands( model_complexity=1,  # Balanced model camera quality 0, 1, 2\n",
    "    min_detection_confidence=0.5,  # confidence for detection How sure it must be before saying “that’s a hand”\n",
    "    min_tracking_confidence=0.5,   # confidence for tracking How sure it must be to keep following the hand\n",
    "    max_num_hands=4  # maximum number of hands to detect\n",
    "    )\n",
    "\n",
    "custom_landmark_style = mp_drawing.DrawingSpec(color=(161, 3, 11), thickness=2, circle_radius=4)\n",
    "custom_connection_style = mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=1)\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "while cam.isOpened():\n",
    "    success, image = cam.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the image and find hands\n",
    "    results = hands.process(image)\n",
    "    # Convert the image back to BGR for display\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw hand landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS, custom_landmark_style, custom_connection_style)\n",
    "    # Display the result\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1)) # Flip the image horizontally for a selfie-view display.\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d95eb7",
   "metadata": {},
   "source": [
    "OpenCV needs waitKey because without an event loop, windows are dead objects.\n",
    "Matplotlib doesn’t because it embeds itself inside one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05897baf",
   "metadata": {},
   "source": [
    "| Key   | ASCII code |\n",
    "| ----- | ---------- |\n",
    "| ESC   | 27         |\n",
    "| Space | 32         |\n",
    "| A     | 65         |\n",
    "| a     | 97         |\n",
    "| Enter | 13         |\n",
    "\n",
    "if cv2.waitKey(1) != -1:  # any key pressed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ae806",
   "metadata": {},
   "source": [
    "## <font>Adding Text</font>\n",
    "\n",
    "\n",
    "we can write some text on an image using **cv2.putText** function.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Functional syntx</font>\n",
    "\n",
    "    img = cv2.putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
    "\n",
    "`img`: The output image that has been annotated.\n",
    "\n",
    "The function has **6 required arguments**:\n",
    "\n",
    "1. `img`: Image on which the text has to be written.\n",
    "\n",
    "2. `text`: Text string to be written.\n",
    "\n",
    "3. `org`: Bottom-left corner of the text string in the image.\n",
    "\n",
    "4. `fontFace`: Font type\n",
    "\n",
    "5. `fontScale`: Font scale factor that is multiplied by the font-specific base size. when negative it flipes the text\n",
    "\n",
    "6. `color`: Font color\n",
    "\n",
    "Other optional arguments that are important for us to know include:\n",
    "\n",
    "1. `thickness`: Integer specifying the line thickness for the text. Default value is 1.\n",
    "\n",
    "2. `lineType`: Type of the line. Default value is 8 which stands for an 8-connected line. Usually, cv2.LINE_AA (antialiased or smooth line) is used for the lineType.\n",
    "3. `bottomLeftOrigin`\tWhen true, the image data origin is at the bottom-left corner. Otherwise, it is at the top-left corner.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">OpenCV Documentation</font>\n",
    "\n",
    "**`putText`**: <a href=\"https://docs.opencv.org/4.5.1/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576\" target=\"_blank\">Documentation link</a>\n",
    "    \n",
    "Let's see an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e75932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands( model_complexity=1,  # Balanced model camera quality 0, 1, 2\n",
    "    min_detection_confidence=0.5,  # confidence for detection How sure it must be before saying “that’s a hand”\n",
    "    min_tracking_confidence=0.5,   # confidence for tracking How sure it must be to keep following the hand\n",
    "    max_num_hands=4  # maximum number of hands to detect\n",
    "    )\n",
    "\n",
    "custom_landmark_style = mp_drawing.DrawingSpec(color=(161, 3, 11), thickness=2, circle_radius=4)\n",
    "custom_connection_style = mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=1)\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "while cam.isOpened():\n",
    "    success, image = cam.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Process the image and find hands\n",
    "    results = hands.process(image)\n",
    "    # Convert the image back to BGR for display\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw hand landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS, custom_landmark_style, custom_connection_style)\n",
    "    number_of_fingers = 0\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Thumb\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y < hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP].y:\n",
    "                number_of_fingers += 1\n",
    "            # Index Finger\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y < hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP].y:\n",
    "                number_of_fingers += 1\n",
    "            # Middle Finger\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y < hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP].y:\n",
    "                number_of_fingers += 1\n",
    "            # Ring Finger\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP].y < hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP].y:\n",
    "                number_of_fingers += 1\n",
    "            # Pinky Finger\n",
    "            if hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP].y < hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP].y:\n",
    "                number_of_fingers += 1\n",
    "    # Display number of fingers\n",
    "    cv2.putText(image, f'Fingers: {number_of_fingers}', (400, 40), cv2.FONT_HERSHEY_SIMPLEX, -1, (255, 0, 0), 1, cv2.LINE_AA, bottomLeftOrigin=True)\n",
    "    #cv2.putText(img, text, org, fontFace, fontScale, color, thickness, lineType, bottomLeftOrigin)\n",
    "    # Display the result\n",
    "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1)) # Flip the image horizontally for a selfie-view display.\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
